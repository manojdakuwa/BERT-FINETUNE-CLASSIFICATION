{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d55b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (4.44.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (2.19.1)\n",
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp310-cp310-win_amd64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from torch) (4.13.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mann\\anaconda3\\envs\\llmonclusive\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading torch-2.7.0-cp310-cp310-win_amd64.whl (212.5 MB)\n",
      "   ---------------------------------------- 0.0/212.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/212.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/212.5 MB 2.6 MB/s eta 0:01:20\n",
      "   ---------------------------------------- 1.6/212.5 MB 2.8 MB/s eta 0:01:16\n",
      "   ---------------------------------------- 2.1/212.5 MB 2.8 MB/s eta 0:01:16\n",
      "   ---------------------------------------- 2.6/212.5 MB 2.8 MB/s eta 0:01:16\n",
      "    --------------------------------------- 3.1/212.5 MB 2.7 MB/s eta 0:01:19\n",
      "    --------------------------------------- 3.9/212.5 MB 2.7 MB/s eta 0:01:18\n",
      "    --------------------------------------- 4.5/212.5 MB 2.7 MB/s eta 0:01:17\n",
      "    --------------------------------------- 5.0/212.5 MB 2.7 MB/s eta 0:01:17\n",
      "   - -------------------------------------- 5.8/212.5 MB 2.7 MB/s eta 0:01:16\n",
      "   - -------------------------------------- 6.3/212.5 MB 2.7 MB/s eta 0:01:16\n",
      "   - -------------------------------------- 6.8/212.5 MB 2.7 MB/s eta 0:01:16\n",
      "   - -------------------------------------- 7.3/212.5 MB 2.7 MB/s eta 0:01:15\n",
      "   - -------------------------------------- 8.1/212.5 MB 2.8 MB/s eta 0:01:15\n",
      "   - -------------------------------------- 8.7/212.5 MB 2.8 MB/s eta 0:01:14\n",
      "   - -------------------------------------- 9.2/212.5 MB 2.8 MB/s eta 0:01:14\n",
      "   - -------------------------------------- 10.0/212.5 MB 2.8 MB/s eta 0:01:14\n",
      "   - -------------------------------------- 10.5/212.5 MB 2.8 MB/s eta 0:01:13\n",
      "   -- ------------------------------------- 11.0/212.5 MB 2.8 MB/s eta 0:01:13\n",
      "   -- ------------------------------------- 11.8/212.5 MB 2.8 MB/s eta 0:01:13\n",
      "   -- ------------------------------------- 12.3/212.5 MB 2.8 MB/s eta 0:01:13\n",
      "   -- ------------------------------------- 12.8/212.5 MB 2.8 MB/s eta 0:01:12\n",
      "   -- ------------------------------------- 13.4/212.5 MB 2.8 MB/s eta 0:01:12\n",
      "   -- ------------------------------------- 14.2/212.5 MB 2.8 MB/s eta 0:01:12\n",
      "   -- ------------------------------------- 14.7/212.5 MB 2.8 MB/s eta 0:01:11\n",
      "   -- ------------------------------------- 15.2/212.5 MB 2.8 MB/s eta 0:01:11\n",
      "   --- ------------------------------------ 16.0/212.5 MB 2.8 MB/s eta 0:01:11\n",
      "   --- ------------------------------------ 16.5/212.5 MB 2.8 MB/s eta 0:01:11\n",
      "   --- ------------------------------------ 17.0/212.5 MB 2.8 MB/s eta 0:01:10\n",
      "   --- ------------------------------------ 17.8/212.5 MB 2.8 MB/s eta 0:01:10\n",
      "   --- ------------------------------------ 18.4/212.5 MB 2.8 MB/s eta 0:01:10\n",
      "   --- ------------------------------------ 18.9/212.5 MB 2.8 MB/s eta 0:01:10\n",
      "   --- ------------------------------------ 19.4/212.5 MB 2.8 MB/s eta 0:01:10\n",
      "   --- ------------------------------------ 19.9/212.5 MB 2.8 MB/s eta 0:01:10\n",
      "   --- ------------------------------------ 20.4/212.5 MB 2.8 MB/s eta 0:01:10\n",
      "   --- ------------------------------------ 21.0/212.5 MB 2.7 MB/s eta 0:01:10\n",
      "   ---- ----------------------------------- 21.5/212.5 MB 2.8 MB/s eta 0:01:10\n",
      "   ---- ----------------------------------- 22.3/212.5 MB 2.8 MB/s eta 0:01:09\n",
      "   ---- ----------------------------------- 22.8/212.5 MB 2.8 MB/s eta 0:01:09\n",
      "   ---- ----------------------------------- 23.3/212.5 MB 2.8 MB/s eta 0:01:09\n",
      "   ---- ----------------------------------- 24.1/212.5 MB 2.8 MB/s eta 0:01:09\n",
      "   ---- ----------------------------------- 24.6/212.5 MB 2.8 MB/s eta 0:01:08\n",
      "   ---- ----------------------------------- 25.2/212.5 MB 2.8 MB/s eta 0:01:08\n",
      "   ---- ----------------------------------- 26.0/212.5 MB 2.8 MB/s eta 0:01:08\n",
      "   ---- ----------------------------------- 26.5/212.5 MB 2.8 MB/s eta 0:01:08\n",
      "   ----- ---------------------------------- 27.0/212.5 MB 2.8 MB/s eta 0:01:07\n",
      "   ----- ---------------------------------- 27.5/212.5 MB 2.8 MB/s eta 0:01:07\n",
      "   ----- ---------------------------------- 28.3/212.5 MB 2.8 MB/s eta 0:01:07\n",
      "   ----- ---------------------------------- 28.8/212.5 MB 2.8 MB/s eta 0:01:07\n",
      "   ----- ---------------------------------- 29.4/212.5 MB 2.8 MB/s eta 0:01:07\n",
      "   ----- ---------------------------------- 30.1/212.5 MB 2.8 MB/s eta 0:01:06\n",
      "   ----- ---------------------------------- 30.7/212.5 MB 2.8 MB/s eta 0:01:06\n",
      "   ----- ---------------------------------- 30.9/212.5 MB 2.8 MB/s eta 0:01:06\n",
      "   ----- ---------------------------------- 31.5/212.5 MB 2.7 MB/s eta 0:01:07\n",
      "   ------ --------------------------------- 32.0/212.5 MB 2.7 MB/s eta 0:01:06\n",
      "   ------ --------------------------------- 32.5/212.5 MB 2.7 MB/s eta 0:01:06\n",
      "   ------ --------------------------------- 33.0/212.5 MB 2.7 MB/s eta 0:01:06\n",
      "   ------ --------------------------------- 33.6/212.5 MB 2.7 MB/s eta 0:01:06\n",
      "   ------ --------------------------------- 34.1/212.5 MB 2.7 MB/s eta 0:01:06\n",
      "   ------ --------------------------------- 34.9/212.5 MB 2.7 MB/s eta 0:01:06\n",
      "   ------ --------------------------------- 35.4/212.5 MB 2.7 MB/s eta 0:01:05\n",
      "   ------ --------------------------------- 35.9/212.5 MB 2.7 MB/s eta 0:01:05\n",
      "   ------ --------------------------------- 36.4/212.5 MB 2.7 MB/s eta 0:01:05\n",
      "   ------ --------------------------------- 37.0/212.5 MB 2.7 MB/s eta 0:01:05\n",
      "   ------- -------------------------------- 37.7/212.5 MB 2.7 MB/s eta 0:01:05\n",
      "   ------- -------------------------------- 38.3/212.5 MB 2.7 MB/s eta 0:01:05\n",
      "   ------- -------------------------------- 38.8/212.5 MB 2.7 MB/s eta 0:01:04\n",
      "   ------- -------------------------------- 39.6/212.5 MB 2.7 MB/s eta 0:01:04\n",
      "   ------- -------------------------------- 40.1/212.5 MB 2.7 MB/s eta 0:01:04\n",
      "   ------- -------------------------------- 40.4/212.5 MB 2.7 MB/s eta 0:01:04\n",
      "   ------- -------------------------------- 41.2/212.5 MB 2.7 MB/s eta 0:01:04\n",
      "   ------- -------------------------------- 41.4/212.5 MB 2.7 MB/s eta 0:01:04\n",
      "   ------- -------------------------------- 41.9/212.5 MB 2.7 MB/s eta 0:01:04\n",
      "   ------- -------------------------------- 42.2/212.5 MB 2.7 MB/s eta 0:01:04\n",
      "   ------- -------------------------------- 42.5/212.5 MB 2.6 MB/s eta 0:01:05\n",
      "   ------- -------------------------------- 42.5/212.5 MB 2.6 MB/s eta 0:01:05\n",
      "   -------- ------------------------------- 42.7/212.5 MB 2.6 MB/s eta 0:01:06\n",
      "   -------- ------------------------------- 43.0/212.5 MB 2.6 MB/s eta 0:01:06\n",
      "   -------- ------------------------------- 43.3/212.5 MB 2.6 MB/s eta 0:01:06\n",
      "   -------- ------------------------------- 43.5/212.5 MB 2.6 MB/s eta 0:01:07\n",
      "   -------- ------------------------------- 44.3/212.5 MB 2.6 MB/s eta 0:01:06\n",
      "   -------- ------------------------------- 44.8/212.5 MB 2.6 MB/s eta 0:01:06\n",
      "   -------- ------------------------------- 45.4/212.5 MB 2.6 MB/s eta 0:01:06\n",
      "   -------- ------------------------------- 45.9/212.5 MB 2.6 MB/s eta 0:01:06\n",
      "   -------- ------------------------------- 46.7/212.5 MB 2.6 MB/s eta 0:01:05\n",
      "   -------- ------------------------------- 47.2/212.5 MB 2.6 MB/s eta 0:01:05\n",
      "   -------- ------------------------------- 47.7/212.5 MB 2.6 MB/s eta 0:01:05\n",
      "   --------- ------------------------------ 48.2/212.5 MB 2.6 MB/s eta 0:01:04\n",
      "   --------- ------------------------------ 49.0/212.5 MB 2.6 MB/s eta 0:01:04\n",
      "   --------- ------------------------------ 49.5/212.5 MB 2.6 MB/s eta 0:01:04\n",
      "   --------- ------------------------------ 50.1/212.5 MB 2.6 MB/s eta 0:01:04\n",
      "   --------- ------------------------------ 50.6/212.5 MB 2.6 MB/s eta 0:01:03\n",
      "   --------- ------------------------------ 51.4/212.5 MB 2.6 MB/s eta 0:01:03\n",
      "   --------- ------------------------------ 51.9/212.5 MB 2.6 MB/s eta 0:01:03\n",
      "   --------- ------------------------------ 52.4/212.5 MB 2.6 MB/s eta 0:01:02\n",
      "   ---------- ----------------------------- 53.2/212.5 MB 2.6 MB/s eta 0:01:02\n",
      "   ---------- ----------------------------- 53.7/212.5 MB 2.6 MB/s eta 0:01:02\n",
      "   ---------- ----------------------------- 54.3/212.5 MB 2.6 MB/s eta 0:01:02\n",
      "   ---------- ----------------------------- 55.1/212.5 MB 2.6 MB/s eta 0:01:01\n",
      "   ---------- ----------------------------- 55.6/212.5 MB 2.6 MB/s eta 0:01:01\n",
      "   ---------- ----------------------------- 56.1/212.5 MB 2.6 MB/s eta 0:01:01\n",
      "   ---------- ----------------------------- 56.9/212.5 MB 2.6 MB/s eta 0:01:00\n",
      "   ---------- ----------------------------- 57.4/212.5 MB 2.6 MB/s eta 0:01:00\n",
      "   ---------- ----------------------------- 58.2/212.5 MB 2.6 MB/s eta 0:01:00\n",
      "   ----------- ---------------------------- 58.7/212.5 MB 2.6 MB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 59.2/212.5 MB 2.6 MB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 59.8/212.5 MB 2.6 MB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 60.6/212.5 MB 2.6 MB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 61.1/212.5 MB 2.6 MB/s eta 0:00:58\n",
      "   ----------- ---------------------------- 61.9/212.5 MB 2.6 MB/s eta 0:00:58\n",
      "   ----------- ---------------------------- 62.4/212.5 MB 2.6 MB/s eta 0:00:58\n",
      "   ----------- ---------------------------- 62.9/212.5 MB 2.6 MB/s eta 0:00:58\n",
      "   ----------- ---------------------------- 63.4/212.5 MB 2.6 MB/s eta 0:00:57\n",
      "   ------------ --------------------------- 64.2/212.5 MB 2.6 MB/s eta 0:00:57\n",
      "   ------------ --------------------------- 64.7/212.5 MB 2.6 MB/s eta 0:00:57\n",
      "   ------------ --------------------------- 65.3/212.5 MB 2.6 MB/s eta 0:00:56\n",
      "   ------------ --------------------------- 66.1/212.5 MB 2.6 MB/s eta 0:00:56\n",
      "   ------------ --------------------------- 66.6/212.5 MB 2.6 MB/s eta 0:00:56\n",
      "   ------------ --------------------------- 67.1/212.5 MB 2.6 MB/s eta 0:00:56\n",
      "   ------------ --------------------------- 67.9/212.5 MB 2.6 MB/s eta 0:00:55\n",
      "   ------------ --------------------------- 68.4/212.5 MB 2.6 MB/s eta 0:00:55\n",
      "   ------------ --------------------------- 68.9/212.5 MB 2.6 MB/s eta 0:00:55\n",
      "   ------------- -------------------------- 69.5/212.5 MB 2.6 MB/s eta 0:00:55\n",
      "   ------------- -------------------------- 70.0/212.5 MB 2.6 MB/s eta 0:00:55\n",
      "   ------------- -------------------------- 70.5/212.5 MB 2.6 MB/s eta 0:00:54\n",
      "   ------------- -------------------------- 71.0/212.5 MB 2.6 MB/s eta 0:00:54\n",
      "   ------------- -------------------------- 71.6/212.5 MB 2.6 MB/s eta 0:00:54\n",
      "   ------------- -------------------------- 72.4/212.5 MB 2.6 MB/s eta 0:00:54\n",
      "   ------------- -------------------------- 72.6/212.5 MB 2.6 MB/s eta 0:00:54\n",
      "   ------------- -------------------------- 73.1/212.5 MB 2.6 MB/s eta 0:00:53\n",
      "   ------------- -------------------------- 73.9/212.5 MB 2.6 MB/s eta 0:00:53\n",
      "   -------------- ------------------------- 74.4/212.5 MB 2.6 MB/s eta 0:00:53\n",
      "   -------------- ------------------------- 75.0/212.5 MB 2.6 MB/s eta 0:00:53\n",
      "   -------------- ------------------------- 75.5/212.5 MB 2.6 MB/s eta 0:00:53\n",
      "   -------------- ------------------------- 76.0/212.5 MB 2.6 MB/s eta 0:00:52\n",
      "   -------------- ------------------------- 76.5/212.5 MB 2.6 MB/s eta 0:00:52\n",
      "   -------------- ------------------------- 77.3/212.5 MB 2.6 MB/s eta 0:00:52\n",
      "   -------------- ------------------------- 77.9/212.5 MB 2.6 MB/s eta 0:00:52\n",
      "   -------------- ------------------------- 78.4/212.5 MB 2.6 MB/s eta 0:00:51\n",
      "   -------------- ------------------------- 79.2/212.5 MB 2.6 MB/s eta 0:00:51\n",
      "   -------------- ------------------------- 79.7/212.5 MB 2.6 MB/s eta 0:00:51\n",
      "   --------------- ------------------------ 80.2/212.5 MB 2.6 MB/s eta 0:00:51\n",
      "   --------------- ------------------------ 80.7/212.5 MB 2.6 MB/s eta 0:00:51\n",
      "   --------------- ------------------------ 81.3/212.5 MB 2.6 MB/s eta 0:00:50\n",
      "   --------------- ------------------------ 82.1/212.5 MB 2.6 MB/s eta 0:00:50\n",
      "   --------------- ------------------------ 82.6/212.5 MB 2.6 MB/s eta 0:00:50\n",
      "   --------------- ------------------------ 83.1/212.5 MB 2.6 MB/s eta 0:00:50\n",
      "   --------------- ------------------------ 83.6/212.5 MB 2.6 MB/s eta 0:00:49\n",
      "   --------------- ------------------------ 84.4/212.5 MB 2.6 MB/s eta 0:00:49\n",
      "   --------------- ------------------------ 84.9/212.5 MB 2.6 MB/s eta 0:00:49\n",
      "   ---------------- ----------------------- 85.5/212.5 MB 2.6 MB/s eta 0:00:49\n",
      "   ---------------- ----------------------- 86.0/212.5 MB 2.6 MB/s eta 0:00:49\n",
      "   ---------------- ----------------------- 86.2/212.5 MB 2.6 MB/s eta 0:00:49\n",
      "   ---------------- ----------------------- 86.8/212.5 MB 2.6 MB/s eta 0:00:48\n",
      "   ---------------- ----------------------- 87.6/212.5 MB 2.6 MB/s eta 0:00:48\n",
      "   ---------------- ----------------------- 88.1/212.5 MB 2.6 MB/s eta 0:00:48\n",
      "   ---------------- ----------------------- 88.6/212.5 MB 2.6 MB/s eta 0:00:48\n",
      "   ---------------- ----------------------- 89.4/212.5 MB 2.6 MB/s eta 0:00:47\n",
      "   ---------------- ----------------------- 89.9/212.5 MB 2.6 MB/s eta 0:00:47\n",
      "   ----------------- ---------------------- 90.4/212.5 MB 2.6 MB/s eta 0:00:47\n",
      "   ----------------- ---------------------- 91.0/212.5 MB 2.6 MB/s eta 0:00:47\n",
      "   ----------------- ---------------------- 91.8/212.5 MB 2.6 MB/s eta 0:00:47\n",
      "   ----------------- ---------------------- 92.3/212.5 MB 2.6 MB/s eta 0:00:46\n",
      "   ----------------- ---------------------- 92.8/212.5 MB 2.6 MB/s eta 0:00:46\n",
      "   ----------------- ---------------------- 93.6/212.5 MB 2.6 MB/s eta 0:00:46\n",
      "   ----------------- ---------------------- 94.1/212.5 MB 2.6 MB/s eta 0:00:46\n",
      "   ----------------- ---------------------- 94.6/212.5 MB 2.6 MB/s eta 0:00:45\n",
      "   ----------------- ---------------------- 95.2/212.5 MB 2.6 MB/s eta 0:00:45\n",
      "   ------------------ --------------------- 95.9/212.5 MB 2.6 MB/s eta 0:00:45\n",
      "   ------------------ --------------------- 96.5/212.5 MB 2.6 MB/s eta 0:00:45\n",
      "   ------------------ --------------------- 97.0/212.5 MB 2.6 MB/s eta 0:00:45\n",
      "   ------------------ --------------------- 97.8/212.5 MB 2.6 MB/s eta 0:00:44\n",
      "   ------------------ --------------------- 98.3/212.5 MB 2.6 MB/s eta 0:00:44\n",
      "   ------------------ --------------------- 98.8/212.5 MB 2.6 MB/s eta 0:00:44\n",
      "   ------------------ --------------------- 99.6/212.5 MB 2.6 MB/s eta 0:00:43\n",
      "   ------------------ --------------------- 100.1/212.5 MB 2.6 MB/s eta 0:00:43\n",
      "   ------------------ --------------------- 100.7/212.5 MB 2.6 MB/s eta 0:00:43\n",
      "   ------------------- -------------------- 101.2/212.5 MB 2.6 MB/s eta 0:00:43\n",
      "   ------------------- -------------------- 102.0/212.5 MB 2.6 MB/s eta 0:00:43\n",
      "   ------------------- -------------------- 102.5/212.5 MB 2.6 MB/s eta 0:00:42\n",
      "   ------------------- -------------------- 103.0/212.5 MB 2.6 MB/s eta 0:00:42\n",
      "   ------------------- -------------------- 103.8/212.5 MB 2.6 MB/s eta 0:00:42\n",
      "   ------------------- -------------------- 104.3/212.5 MB 2.6 MB/s eta 0:00:42\n",
      "   ------------------- -------------------- 104.9/212.5 MB 2.6 MB/s eta 0:00:41\n",
      "   ------------------- -------------------- 105.4/212.5 MB 2.6 MB/s eta 0:00:41\n",
      "   ------------------- -------------------- 106.2/212.5 MB 2.6 MB/s eta 0:00:41\n",
      "   -------------------- ------------------- 106.7/212.5 MB 2.6 MB/s eta 0:00:41\n",
      "   -------------------- ------------------- 107.5/212.5 MB 2.6 MB/s eta 0:00:40\n",
      "   -------------------- ------------------- 108.0/212.5 MB 2.6 MB/s eta 0:00:40\n",
      "   -------------------- ------------------- 108.5/212.5 MB 2.6 MB/s eta 0:00:40\n",
      "   -------------------- ------------------- 109.1/212.5 MB 2.6 MB/s eta 0:00:40\n",
      "   -------------------- ------------------- 109.8/212.5 MB 2.6 MB/s eta 0:00:39\n",
      "   -------------------- ------------------- 110.4/212.5 MB 2.7 MB/s eta 0:00:39\n",
      "   -------------------- ------------------- 110.9/212.5 MB 2.6 MB/s eta 0:00:39\n",
      "   --------------------- ------------------ 111.7/212.5 MB 2.6 MB/s eta 0:00:39\n",
      "   --------------------- ------------------ 112.2/212.5 MB 2.7 MB/s eta 0:00:38\n",
      "   --------------------- ------------------ 112.7/212.5 MB 2.7 MB/s eta 0:00:38\n",
      "   --------------------- ------------------ 113.2/212.5 MB 2.7 MB/s eta 0:00:38\n",
      "   --------------------- ------------------ 114.0/212.5 MB 2.7 MB/s eta 0:00:38\n",
      "   --------------------- ------------------ 114.6/212.5 MB 2.7 MB/s eta 0:00:37\n",
      "   --------------------- ------------------ 115.1/212.5 MB 2.7 MB/s eta 0:00:37\n",
      "   --------------------- ------------------ 115.9/212.5 MB 2.7 MB/s eta 0:00:37\n",
      "   --------------------- ------------------ 116.4/212.5 MB 2.7 MB/s eta 0:00:37\n",
      "   ---------------------- ----------------- 116.9/212.5 MB 2.7 MB/s eta 0:00:36\n",
      "   ---------------------- ----------------- 117.7/212.5 MB 2.7 MB/s eta 0:00:36\n",
      "   ---------------------- ----------------- 118.2/212.5 MB 2.7 MB/s eta 0:00:36\n",
      "   ---------------------- ----------------- 118.8/212.5 MB 2.7 MB/s eta 0:00:36\n",
      "   ---------------------- ----------------- 119.3/212.5 MB 2.7 MB/s eta 0:00:36\n",
      "   ---------------------- ----------------- 120.1/212.5 MB 2.7 MB/s eta 0:00:35\n",
      "   ---------------------- ----------------- 120.6/212.5 MB 2.7 MB/s eta 0:00:35\n",
      "   ---------------------- ----------------- 121.1/212.5 MB 2.7 MB/s eta 0:00:35\n",
      "   ---------------------- ----------------- 121.6/212.5 MB 2.7 MB/s eta 0:00:35\n",
      "   ---------------------- ----------------- 122.2/212.5 MB 2.7 MB/s eta 0:00:34\n",
      "   ----------------------- ---------------- 122.7/212.5 MB 2.7 MB/s eta 0:00:34\n",
      "   ----------------------- ---------------- 123.2/212.5 MB 2.7 MB/s eta 0:00:34\n",
      "   ----------------------- ---------------- 123.7/212.5 MB 2.7 MB/s eta 0:00:33\n",
      "   ----------------------- ---------------- 124.3/212.5 MB 2.7 MB/s eta 0:00:33\n",
      "   ----------------------- ---------------- 125.0/212.5 MB 2.7 MB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 125.6/212.5 MB 2.7 MB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 126.1/212.5 MB 2.8 MB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 126.9/212.5 MB 2.8 MB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 127.4/212.5 MB 2.8 MB/s eta 0:00:31\n",
      "   ------------------------ --------------- 127.9/212.5 MB 2.8 MB/s eta 0:00:31\n",
      "   ------------------------ --------------- 128.5/212.5 MB 2.8 MB/s eta 0:00:31\n",
      "   ------------------------ --------------- 129.2/212.5 MB 2.8 MB/s eta 0:00:31\n",
      "   ------------------------ --------------- 129.8/212.5 MB 2.8 MB/s eta 0:00:31\n",
      "   ------------------------ --------------- 130.3/212.5 MB 2.8 MB/s eta 0:00:30\n",
      "   ------------------------ --------------- 131.1/212.5 MB 2.8 MB/s eta 0:00:30\n",
      "   ------------------------ --------------- 131.6/212.5 MB 2.8 MB/s eta 0:00:30\n",
      "   ------------------------ --------------- 132.1/212.5 MB 2.8 MB/s eta 0:00:30\n",
      "   ------------------------- -------------- 132.9/212.5 MB 2.8 MB/s eta 0:00:29\n",
      "   ------------------------- -------------- 133.4/212.5 MB 2.8 MB/s eta 0:00:29\n",
      "   ------------------------- -------------- 134.0/212.5 MB 2.8 MB/s eta 0:00:29\n",
      "   ------------------------- -------------- 134.7/212.5 MB 2.8 MB/s eta 0:00:29\n",
      "   ------------------------- -------------- 135.3/212.5 MB 2.8 MB/s eta 0:00:28\n",
      "   ------------------------- -------------- 136.1/212.5 MB 2.8 MB/s eta 0:00:28\n",
      "   ------------------------- -------------- 136.6/212.5 MB 2.8 MB/s eta 0:00:28\n",
      "   ------------------------- -------------- 137.1/212.5 MB 2.8 MB/s eta 0:00:28\n",
      "   ------------------------- -------------- 137.6/212.5 MB 2.8 MB/s eta 0:00:28\n",
      "   -------------------------- ------------- 138.4/212.5 MB 2.8 MB/s eta 0:00:27\n",
      "   -------------------------- ------------- 138.9/212.5 MB 2.8 MB/s eta 0:00:27\n",
      "   -------------------------- ------------- 139.5/212.5 MB 2.8 MB/s eta 0:00:27\n",
      "   -------------------------- ------------- 140.2/212.5 MB 2.8 MB/s eta 0:00:27\n",
      "   -------------------------- ------------- 140.8/212.5 MB 2.8 MB/s eta 0:00:27\n",
      "   -------------------------- ------------- 141.3/212.5 MB 2.8 MB/s eta 0:00:26\n",
      "   -------------------------- ------------- 142.1/212.5 MB 2.8 MB/s eta 0:00:26\n",
      "   -------------------------- ------------- 142.6/212.5 MB 2.8 MB/s eta 0:00:26\n",
      "   -------------------------- ------------- 143.1/212.5 MB 2.8 MB/s eta 0:00:26\n",
      "   --------------------------- ------------ 143.7/212.5 MB 2.8 MB/s eta 0:00:25\n",
      "   --------------------------- ------------ 144.4/212.5 MB 2.8 MB/s eta 0:00:25\n",
      "   --------------------------- ------------ 145.0/212.5 MB 2.8 MB/s eta 0:00:25\n",
      "   --------------------------- ------------ 145.5/212.5 MB 2.8 MB/s eta 0:00:25\n",
      "   --------------------------- ------------ 146.3/212.5 MB 2.8 MB/s eta 0:00:25\n",
      "   --------------------------- ------------ 146.8/212.5 MB 2.8 MB/s eta 0:00:24\n",
      "   --------------------------- ------------ 147.3/212.5 MB 2.8 MB/s eta 0:00:24\n",
      "   --------------------------- ------------ 147.8/212.5 MB 2.8 MB/s eta 0:00:24\n",
      "   --------------------------- ------------ 148.6/212.5 MB 2.8 MB/s eta 0:00:24\n",
      "   ---------------------------- ----------- 149.2/212.5 MB 2.8 MB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 149.9/212.5 MB 2.8 MB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 150.5/212.5 MB 2.8 MB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 151.0/212.5 MB 2.8 MB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 151.5/212.5 MB 2.8 MB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 152.3/212.5 MB 2.8 MB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 152.8/212.5 MB 2.8 MB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 153.4/212.5 MB 2.8 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 154.1/212.5 MB 2.8 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 154.7/212.5 MB 2.8 MB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 155.2/212.5 MB 2.8 MB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 156.0/212.5 MB 2.8 MB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 156.5/212.5 MB 2.8 MB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 157.0/212.5 MB 2.8 MB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 157.8/212.5 MB 2.8 MB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 158.3/212.5 MB 2.8 MB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 158.9/212.5 MB 2.8 MB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 159.4/212.5 MB 2.8 MB/s eta 0:00:20\n",
      "   ------------------------------ --------- 160.2/212.5 MB 2.8 MB/s eta 0:00:19\n",
      "   ------------------------------ --------- 160.7/212.5 MB 2.8 MB/s eta 0:00:19\n",
      "   ------------------------------ --------- 161.2/212.5 MB 2.8 MB/s eta 0:00:19\n",
      "   ------------------------------ --------- 162.0/212.5 MB 2.8 MB/s eta 0:00:19\n",
      "   ------------------------------ --------- 162.5/212.5 MB 2.8 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 163.1/212.5 MB 2.8 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 163.8/212.5 MB 2.8 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 164.4/212.5 MB 2.8 MB/s eta 0:00:18\n",
      "   ------------------------------- -------- 164.9/212.5 MB 2.8 MB/s eta 0:00:18\n",
      "   ------------------------------- -------- 165.7/212.5 MB 2.8 MB/s eta 0:00:17\n",
      "   ------------------------------- -------- 166.2/212.5 MB 2.8 MB/s eta 0:00:17\n",
      "   ------------------------------- -------- 167.0/212.5 MB 2.8 MB/s eta 0:00:17\n",
      "   ------------------------------- -------- 167.5/212.5 MB 2.8 MB/s eta 0:00:17\n",
      "   ------------------------------- -------- 168.0/212.5 MB 2.8 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 168.6/212.5 MB 2.8 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 169.3/212.5 MB 2.8 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 169.9/212.5 MB 2.8 MB/s eta 0:00:16\n",
      "   -------------------------------- ------- 170.4/212.5 MB 2.8 MB/s eta 0:00:16\n",
      "   -------------------------------- ------- 171.2/212.5 MB 2.8 MB/s eta 0:00:15\n",
      "   -------------------------------- ------- 171.7/212.5 MB 2.8 MB/s eta 0:00:15\n",
      "   -------------------------------- ------- 172.2/212.5 MB 2.8 MB/s eta 0:00:15\n",
      "   -------------------------------- ------- 173.0/212.5 MB 2.8 MB/s eta 0:00:15\n",
      "   -------------------------------- ------- 173.5/212.5 MB 2.8 MB/s eta 0:00:14\n",
      "   -------------------------------- ------- 174.1/212.5 MB 2.8 MB/s eta 0:00:14\n",
      "   -------------------------------- ------- 174.6/212.5 MB 2.8 MB/s eta 0:00:14\n",
      "   --------------------------------- ------ 175.4/212.5 MB 2.8 MB/s eta 0:00:14\n",
      "   --------------------------------- ------ 175.9/212.5 MB 2.8 MB/s eta 0:00:14\n",
      "   --------------------------------- ------ 176.4/212.5 MB 2.8 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 177.2/212.5 MB 2.8 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 177.7/212.5 MB 2.8 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 178.3/212.5 MB 2.8 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 179.0/212.5 MB 2.8 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 179.6/212.5 MB 2.8 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 180.1/212.5 MB 2.8 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 180.9/212.5 MB 2.8 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 181.4/212.5 MB 2.8 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 181.9/212.5 MB 2.8 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 182.5/212.5 MB 2.8 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 183.2/212.5 MB 2.8 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 183.8/212.5 MB 2.8 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 184.3/212.5 MB 2.8 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 185.1/212.5 MB 2.8 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 185.6/212.5 MB 2.8 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 186.1/212.5 MB 2.8 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 186.6/212.5 MB 2.8 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 187.2/212.5 MB 2.8 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 188.0/212.5 MB 2.8 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 188.5/212.5 MB 2.8 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 189.3/212.5 MB 2.8 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 189.8/212.5 MB 2.8 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 190.3/212.5 MB 2.8 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 191.1/212.5 MB 2.8 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 191.6/212.5 MB 2.8 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 192.2/212.5 MB 2.8 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 192.9/212.5 MB 2.8 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 193.5/212.5 MB 2.8 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 194.0/212.5 MB 2.8 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 194.8/212.5 MB 2.8 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 195.3/212.5 MB 2.8 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 195.8/212.5 MB 2.8 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 196.6/212.5 MB 2.8 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 197.1/212.5 MB 2.8 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 197.7/212.5 MB 2.8 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 198.2/212.5 MB 2.8 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 199.0/212.5 MB 2.8 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 199.5/212.5 MB 2.8 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 200.0/212.5 MB 2.8 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 200.8/212.5 MB 2.8 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 201.3/212.5 MB 2.8 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 202.1/212.5 MB 2.8 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 202.6/212.5 MB 2.8 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 203.2/212.5 MB 2.8 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 203.9/212.5 MB 2.8 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 204.5/212.5 MB 2.8 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 205.0/212.5 MB 2.8 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 205.5/212.5 MB 2.8 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 206.3/212.5 MB 2.8 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 206.8/212.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ---------------------------------------  207.4/212.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------  208.1/212.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------  208.7/212.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------  209.2/212.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------  210.0/212.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  210.5/212.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  211.3/212.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  211.8/212.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  212.3/212.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  212.3/212.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  212.3/212.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 212.5/212.5 MB 2.8 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.3 MB 2.8 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.0/6.3 MB 2.8 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.6/6.3 MB 2.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.4/6.3 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.9/6.3 MB 2.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.4/6.3 MB 2.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 4.2/6.3 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 2.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.3 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 2.8 MB/s eta 0:00:00\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.2 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 sympy-1.14.0 torch-2.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3e2d1c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f0ff1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0623ebb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be390211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Low-risk borrowers (e.g., responsible financial behavior)\n",
    "#- Medium-risk borrowers (e.g., occasional payment struggles)\n",
    "#- High-risk borrowers (e.g., history of defaults or financial instability)\n",
    "# We’ll modify synthetic applicant statements accordingly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5131a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset saved as 'applicants_updated.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define possible applicant statements based on financial risk\n",
    "low_risk_statements = [\n",
    "    \"I always pay my bills on time and have a stable income.\",\n",
    "    \"My savings are strong, and I manage expenses carefully.\",\n",
    "    \"I have a long history of successfully repaying loans.\"\n",
    "]\n",
    "\n",
    "medium_risk_statements = [\n",
    "    \"I sometimes struggle with payments but always catch up.\",\n",
    "    \"My income is stable, but unexpected expenses can be challenging.\",\n",
    "    \"I've taken multiple loans before and repaid most of them on time.\"\n",
    "]\n",
    "\n",
    "high_risk_statements = [\n",
    "    \"I have missed payments in the past and struggled with debt.\",\n",
    "    \"I recently lost my job and have outstanding loans.\",\n",
    "    \"My credit score dropped significantly last year due to financial hardship.\"\n",
    "]\n",
    "\n",
    "# Assign statements based on the default probability\n",
    "np.random.seed(42)\n",
    "risk_categories = np.random.choice(['low', 'medium', 'high'], 10000, p=[0.6, 0.3, 0.1])\n",
    "\n",
    "applicant_statements = []\n",
    "for risk in risk_categories:\n",
    "    if risk == 'low':\n",
    "        applicant_statements.append(np.random.choice(low_risk_statements))\n",
    "    elif risk == 'medium':\n",
    "        applicant_statements.append(np.random.choice(medium_risk_statements))\n",
    "    else:\n",
    "        applicant_statements.append(np.random.choice(high_risk_statements))\n",
    "\n",
    "# Load dataset and apply new statements\n",
    "df = pd.read_csv(\"applicants.csv\")\n",
    "df[\"applicant_statement\"] = applicant_statements\n",
    "\n",
    "# Save refined dataset\n",
    "df.to_csv(\"applicants_updated.csv\", index=False)\n",
    "print(\"Updated dataset saved as 'applicants_updated.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "716ffa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 125.9981\n",
      "Epoch 2: Loss = 124.8095\n",
      "Epoch 3: Loss = 124.6209\n",
      "Fine-tuned BERT model saved!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load updated dataset\n",
    "df = pd.read_csv(\"applicants_updated.csv\")\n",
    "\n",
    "# Define tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Encode applicant statements\n",
    "X_texts = df[\"applicant_statement\"].tolist()\n",
    "y_labels = df[\"default\"].values\n",
    "\n",
    "# Tokenize dataset\n",
    "tokens = tokenizer(X_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=50)\n",
    "X_train_tokens, X_test_tokens, y_train, y_test = train_test_split(tokens[\"input_ids\"], y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define dataset class\n",
    "class LoanDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input_ids\": self.inputs[idx], \"labels\": torch.tensor(self.labels[idx])}\n",
    "\n",
    "# Create data loaders\n",
    "train_data = LoanDataset(X_train_tokens, y_train)\n",
    "test_data = LoanDataset(X_test_tokens, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "# Load pretrained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune BERT model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(3):  # Fine-tune for 3 epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = model(input_ids)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Loss = {total_loss:.4f}\")\n",
    "\n",
    "# Save fine-tuned model\n",
    "model.save_pretrained(\"bert_loan_default_model\")\n",
    "print(\"Fine-tuned BERT model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c03b816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bert_embedding(text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=50)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    return outputs.logits.numpy()  # Use BERT classifier outputs as embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0cd0119",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplicant_statement\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_bert_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m bert_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Merge embeddings into final dataset\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplicant_statement\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mextract_bert_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m bert_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Merge embeddings into final dataset\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m, in \u001b[0;36mextract_bert_embedding\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 4\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokens)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1695\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1691\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1692\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1693\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1696\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1701\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1702\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1703\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1705\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1707\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1709\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    685\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    691\u001b[0m         output_attentions,\n\u001b[0;32m    692\u001b[0m     )\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:626\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    623\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    624\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 626\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\transformers\\pytorch_utils.py:239\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    638\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 639\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    551\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m--> 552\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mann\\anaconda3\\envs\\llmOnclusive\\lib\\site-packages\\torch\\nn\\functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1426\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df[\"bert_embedding\"] = df[\"applicant_statement\"].apply(lambda x: extract_bert_embedding(str(x)))\n",
    "bert_features = np.vstack(df[\"bert_embedding\"].values)\n",
    "\n",
    "# Merge embeddings into final dataset\n",
    "X_final = np.hstack((df.drop(columns=[\"applicant_statement\"]).to_numpy(), bert_features))\n",
    "\n",
    "# Save final dataset with BERT features\n",
    "np.save(\"X_final.npy\", X_final)\n",
    "np.save(\"y_final.npy\", y_labels)\n",
    "\n",
    "print(\"Final dataset saved with BERT embeddings!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12052e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for lambda_reg in lambda_values:\n",
    "        print(f\"Training Logistic Regression with Fine-Tuned BERT → lr={lr}, lambda={lambda_reg}\")\n",
    "        w, losses = batch_gradient_descent(X_train, y_train, lr, lambda_reg, epochs=1000)\n",
    "        results[(lr, lambda_reg)] = (w, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa8424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "\n",
    "\n",
    "for (lr, lambda_reg), (w, _) in results.items():\n",
    "    y_pred_test = predict(X_test, w)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    precision = precision_score(y_test, y_pred_test)\n",
    "    recall = recall_score(y_test, y_pred_test)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    print(f\"Fine-Tuned BERT Model → lr={lr}, lambda={lambda_reg}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, ROC-AUC: {roc_auc:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmOnclusive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
